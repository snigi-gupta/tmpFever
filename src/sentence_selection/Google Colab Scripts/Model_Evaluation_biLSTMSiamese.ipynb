{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_Evaluation_biLSTMSiamese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGlhKPlB7GkZ",
        "colab_type": "text"
      },
      "source": [
        "# **Model Evaluation** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmFq7fnz7N8l",
        "colab_type": "code",
        "outputId": "67822a39-ac0a-440c-b8dc-d76c93bf28f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!pip install fever-scorer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fever-scorer in /usr/local/lib/python3.6/dist-packages (2.0.39)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fever-scorer) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5piQ5_Rt7Kp3",
        "colab_type": "code",
        "outputId": "11c6da09-9893-41b6-b17f-c6a67492b14e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvB0WY217RcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "import torch.nn.functional as tnf\n",
        "from fever.scorer import fever_score\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import dill"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf7lzS-Y7i-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEn_BYmP7CRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_path = \"/content/gdrive/My Drive/NLPWikiData/processed_dev_data3.csv\"\n",
        "org_dev_path = \"/content/gdrive/My Drive/NLPWikiData/dev.jsonl\"\n",
        "model_path = \"/content/gdrive/My Drive/sent_selec_E4_0.425.pt\"\n",
        "sen_pred_dev_path = \"/content/gdrive/My Drive/NLPWikiData/sen_pred_dev3.jsonl\"\n",
        "dev_fever_data_path = \"/content/gdrive/My Drive/NLPWikiData/sen_pred_fever_dev3.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mXGuail7hRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(include_lengths = True, tokenize='spacy')\n",
        "LABEL = data.LabelField()\n",
        "OTHER = data.RawField()\n",
        "OTHER.is_target = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caiQc2dyDfPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "devset_fields = {\"sentence\":(\"sentence\",TEXT), \"claim\":(\"claim\", TEXT), \n",
        "                 \"org_sentence\":(\"org_sentence\",OTHER), \"docid_claimid_sentno\":(\"docid_claimid_sentno\",OTHER)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkFvB5gq7zjf",
        "colab_type": "code",
        "outputId": "ea2d4644-3e15-47ec-d237-aa0c0b18b1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "with open(\"/content/gdrive/My Drive/TEXT_VOCAB_5EPOCH\", \"rb\") as f:\n",
        "    TEST_TEXT = dill.load(f)\n",
        "    print(\"Text Load Successfull\")\n",
        "with open(\"/content/gdrive/My Drive/LABEL_VOCAB_5EPOCH\", \"rb\") as f:\n",
        "    TEST_LABEL = dill.load(f)\n",
        "    print(\"Label Load Successfull\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text Load Successfull\n",
            "Label Load Successfull\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5OvClDB74zv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "devset = data.TabularDataset(dev_path, format=\"CSV\", fields=devset_fields, skip_header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJmtlegK77Mz",
        "colab_type": "code",
        "outputId": "64c1b187-7705-4180-a482-4f10641560cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(len(devset))\n",
        "print(vars(devset.examples[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "992977\n",
            "{'sentence': ['Colin', 'Rand', 'Kaepernick', '-LRB-', '-LSB-', '`', 'kæpərnɪk', '-RSB-', ';', 'born', 'November', '3', ',', '1987', '-RRB-', 'is', 'an', 'American', 'football', 'quarterback', 'who', 'is', 'currently', 'a', 'free', 'agent', '.'], 'claim': ['Colin', 'Kaepernick', 'became', 'a', 'starting', 'quarterback', 'during', 'the', '49ers', '63rd', 'season', 'in', 'the', 'National', 'Football', 'League', '.'], 'org_sentence': 'Colin Rand Kaepernick -LRB- -LSB- ` kæpərnɪk -RSB- ; born November 3 , 1987 -RRB- is an American football quarterback who is currently a free agent .', 'docid_claimid_sentno': 'Colin_Kaepernick{#--#}91198{#--#}0'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHK3k38X79-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(devset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V8wQIZs7_LN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.build_vocab(devset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDy-BBnF7_IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.vocab = TEST_TEXT.vocab\n",
        "TEXT.vocab.itos = TEST_TEXT.vocab.itos\n",
        "TEXT.vocab.stoi = TEST_TEXT.vocab.stoi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1sW1lMh7_Fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.vocab = TEST_LABEL.vocab\n",
        "LABEL.vocab.itos = TEST_LABEL.vocab.itos\n",
        "LABEL.vocab.stoi = TEST_LABEL.vocab.stoi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJuLxdeb7_Bh",
        "colab_type": "code",
        "outputId": "ba947668-1bc2-43af-ee65-d412e7473fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# vocabulary of training data (same to be used for dev and test)\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 90622\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfCNx_Qu7-3G",
        "colab_type": "code",
        "outputId": "0d0a2d8c-c652-495e-9a5b-a681da46450f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))\n",
        "print(TEXT.vocab.itos[:10])\n",
        "print(vars(LABEL.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 6679593), (',', 5993860), ('the', 5457627), ('in', 3084025), ('and', 3053898), ('of', 2935330), ('a', 2786757), ('is', 2062525), ('was', 1467657), ('to', 1264993), ('The', 1254807), ('-LRB-', 1104248), ('-RRB-', 1104213), ('-', 1028424), ('for', 960542), ('as', 863462), (\"'s\", 799846), ('by', 771416), ('`', 761885), ('an', 748843)]\n",
            "['<unk>', '<pad>', '.', ',', 'the', 'in', 'and', 'of', 'a', 'is']\n",
            "{'freqs': Counter({'False': 3082707, 'True': 264198}), 'itos': ['False', 'True'], 'stoi': defaultdict(<function _default_unk_index at 0x7ff243702950>, {'False': 0, 'True': 1}), 'vectors': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72lkv7gg7-qd",
        "colab_type": "code",
        "outputId": "34f37174-adc0-49a9-8ca6-b18516742e1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE=128\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"We are working with \", device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are working with  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLn6kh2e8ON5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_iterator = data.BucketIterator(\n",
        "    devset, \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    sort_key = lambda x: (len(x.claim)),\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2lgTj3g8Sss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
        "        self.fc = nn.Linear(hidden_dim*2*2, output_dim)\n",
        "\n",
        "        # self.dropoutVar = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward_again(self, text, text_lengths):\n",
        "        \n",
        "        # print(text)\n",
        "        # text = [sent_len, batch_size]\n",
        "        # print(\"Text_Shape:  \",text.shape)\n",
        "        # print(\"Text_Length: \",text_lengths)\n",
        "        # print(\"Text_Length_Shape: \",text_lengths.shape)\n",
        "\n",
        "        output = self.embedding(text) #get embeddings\n",
        "        pps = nn.utils.rnn.pack_padded_sequence(output, text_lengths, enforce_sorted=False) #perform packed padded sequence\n",
        "        output2, (hiddenLSTM, cellLSTM) = self.lstm(pps) #lstm\n",
        "        hidden = torch.cat((hiddenLSTM[-2,:,:], hiddenLSTM[-1,:,:]),1) #get concatenated hidden\n",
        "\n",
        "        # print(\"Output:  \",output)\n",
        "        # print(\"Output_Shape:  \",output.shape)\n",
        "        \n",
        "        # print(\"PPS:  \",pps)\n",
        "        # print(\"PPS_Shape:  \",pps.shape)\n",
        "\n",
        "        # print(\"Output2:  \",output2)\n",
        "        # print(\"Output2_Shape:  \",output2.shape)\n",
        "        \n",
        "        # print(\"Hidden:  \",hidden)\n",
        "        # print(\"Hidden_Shape:  \",hidden.shape)\n",
        "        \n",
        "        return hidden\n",
        "\n",
        "    def forward(self, claims, sentences):\n",
        "        claim_text = claims[0]\n",
        "        claim_text_length = claims[1]\n",
        "        sentence_text = sentences[0]\n",
        "        sentence_text_length = sentences[1]\n",
        "\n",
        "        claim_hidden = self.forward_again(claim_text, claim_text_length)\n",
        "        sentence_hidden = self.forward_again(sentence_text, sentence_text_length)\n",
        "\n",
        "        concatenated_hidden = torch.cat((claim_hidden,sentence_hidden), 1)\n",
        "\n",
        "        return self.fc(concatenated_hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15P_0EN38XAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 2\n",
        "N_LAYERS = 1\n",
        "BIDIRECTIONAL = True\n",
        "# DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = LSTM(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrMV54G28aDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "model.load_state_dict(torch.load(model_path, map_location=device)) \n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsAKaK9L89Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, file):\n",
        "  \n",
        "    epoch_loss = 0\n",
        "\n",
        "    # doc_ids = []\n",
        "    # sentence_nos = []\n",
        "    # claim_ids = []\n",
        "    docid_claimid_sentno = []\n",
        "    org_sentences = []\n",
        "    predicted_sentences = []\n",
        "    probabilities = []\n",
        "    correct_predictions = 0\n",
        "    total_claims = 0\n",
        "\n",
        "    # epoch_acc = 0\n",
        "\n",
        "  \n",
        "  \n",
        "    with torch.no_grad():\n",
        "  \n",
        "        for i, batch in enumerate(iterator):\n",
        "            model.eval()\n",
        "            \n",
        "            claims, sentences = batch.claim, batch.sentence\n",
        "            \n",
        "            eval_predictions = model(claims, sentences)\n",
        "            probability = tnf.softmax(eval_predictions, 1)\n",
        "            # correct_predictions += (torch.max(eval_predictions, 1)[1].view(batch.sent_label.size()) == batch.sent_label).sum().item()\n",
        "            # dev_loss = criterion(eval_predictions, batch.sent_label)\n",
        "            \n",
        "            # epoch_loss += dev_loss.item()\n",
        "            # total_claims += batch.sent_label.size(0)\n",
        "            # epoch_acc += acc.item()\n",
        "\n",
        "            # predicted_sentences.extend(eval_predictions[:,1].tolist())\n",
        "            probabilities.extend(probability[:,1].tolist())\n",
        "            docid_claimid_sentno.extend(batch.docid_claimid_sentno)\n",
        "            org_sentences.extend(batch.org_sentence)\n",
        "\n",
        "        file_data, fever_data = get_score_test(probabilities, docid_claimid_sentno, org_sentences, org_dev_path)   \n",
        "        # average_accuracy = 100. * correct_predictions / total_claims\n",
        "        # print(f'Correct Predictions: {correct_predictions}')\n",
        "        # print(f'Total Claims: {total_claims}')\n",
        "        # print(f'Validation Loss: {epoch_loss/len(iterator)}')\n",
        "        # print(f'Average Accuracy: {average_accuracy}%')\n",
        "        print(f'-----------------------------')\n",
        "\n",
        "    return file_data, fever_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tDZRjFP8sLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_score_test(probabilities, docid_claimid_sentno, org_sentence, org_dev_path):\n",
        "    org_dev_data = pd.read_json(org_dev_path, lines=True)\n",
        "\n",
        "    claim_dict = dict()\n",
        "\n",
        "    for i,val in enumerate(docid_claimid_sentno):\n",
        "        doc_id, claim_id, sentno = docid_claimid_sentno[i].split(\"{#--#}\")\n",
        "        claim_id = int(claim_id)\n",
        "        if claim_id not in claim_dict:\n",
        "            claim_dict[claim_id] = [{\"probability\": probabilities[i], \"doc_id\": doc_id, \n",
        "                                    \"sentno\": sentno, \"org_sentence\": org_sentence[i]}]\n",
        "        else:\n",
        "            claim_dict[claim_id].extend([{\"probability\": probabilities[i], \"doc_id\": doc_id, \n",
        "                                    \"sentno\": sentno, \"org_sentence\": org_sentence[i]}])\n",
        "    \n",
        "    file_data = []\n",
        "    fever_data = []\n",
        "    prob_count = 0\n",
        "    # pdb.set_trace()\n",
        "    for org_dev_claim_id, org_dev_claim, org_dev_claim_label, org_dev_evidence_list in zip(org_dev_data['id'], \n",
        "                                                                                           org_dev_data['claim'], \n",
        "                                                                                           org_dev_data['label'],\n",
        "                                                                                           org_dev_data['evidence']):\n",
        "        temp_data = dict()\n",
        "        fever_dict = dict()\n",
        "\n",
        "        org_dev_claim_id = int(org_dev_claim_id)\n",
        "        predicted_sentences = []\n",
        "        if org_dev_claim_id not in claim_dict:\n",
        "            print(f'Claim id not found   {org_dev_claim_id}')\n",
        "            # that claim id was not in the predictions, hence no predicted sentences\n",
        "            predicted_sentences = []\n",
        "\n",
        "            # for RTE .jsonl file\n",
        "            temp_data['id'] = org_dev_claim_id\n",
        "            temp_data['claim'] = org_dev_claim\n",
        "            temp_data['sentences'] = []\n",
        "            temp_data['page_ids'] = []\n",
        "            temp_data['indices'] = []\n",
        "\n",
        "\n",
        "            \n",
        "        else:\n",
        "            the_claim_dict = claim_dict[org_dev_claim_id]\n",
        "            for value in the_claim_dict:\n",
        "                #   if value['probability'] >= 0.5:\n",
        "                # print (predicted_sentences)\n",
        "                # print (value)\n",
        "                prob_count +=1\n",
        "                predicted_sentences.append([value['probability'], value['sentno'], value['org_sentence'], value['doc_id']])\n",
        "                            \n",
        "            sorted_predicted_sentences = sorted(predicted_sentences, key=lambda x: x[0], reverse=True)\n",
        "                \n",
        "            # for RTE .jsonl file\n",
        "            temp_data['id'] = org_dev_claim_id\n",
        "            temp_data['claim'] = org_dev_claim\n",
        "            temp_data['sentences'] = [u[2] for u in sorted_predicted_sentences][:5]\n",
        "            temp_data['page_ids'] = [v[3] for v in sorted_predicted_sentences][:5]\n",
        "            temp_data['indices'] = [w[1] for w in sorted_predicted_sentences][:5]\n",
        "            \n",
        "\n",
        "            # for fever score\n",
        "            fever_dict['label'] = org_dev_claim_label\n",
        "            fever_dict['predicted_label'] = org_dev_claim_label\n",
        "            fever_dict['predicted_evidence'] = [[x[3], int(x[1])] for x in sorted_predicted_sentences][:5]\n",
        "            fever_dict['evidence'] = org_dev_evidence_list\n",
        "            # fever_dict = \"No fever Data\"\n",
        "\n",
        "        file_data.append(temp_data)\n",
        "        fever_data.append(fever_dict)\n",
        "    # pd.DataFrame(file_data).to_json(sen_pred_test_path, orient='records', lines=True)\n",
        "    print('prob_count', prob_count)\n",
        "    return file_data, fever_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFURQcv19Gae",
        "colab_type": "code",
        "outputId": "b44a2083-c6f6-4db8-939a-9756c9902dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "dev_file_data, dev_fever_data = evaluate(model, dev_iterator, dev_path)\n",
        "pd.DataFrame(dev_file_data).to_json(sen_pred_dev_path, orient='records', lines=True)\n",
        "pd.DataFrame(dev_fever_data).to_csv(dev_fever_data_path)\n",
        "print(\"Done!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prob_count 992977\n",
            "-----------------------------\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NAQW03bAGVL",
        "colab_type": "code",
        "outputId": "80a7fc9c-d333-4ce5-f167-350b2858221a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "dev_fever_val, dev_accuracy, dev_precision, dev_recall, f1score = fever_score(dev_fever_data)\n",
        "print(f'Fever Score: {dev_fever_val} | Accuracy: {dev_accuracy}')\n",
        "print(f'Precision: {dev_precision} | Recall: {dev_recall} | F1Score: {f1score}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fever Score: 0.49364936493649364 | Accuracy: 1.0\n",
            "Precision: 0.06352760276027923 | Recall: 0.2404740474047405 | F1Score: 0.1005043212718899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjK8owcRRcJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = pd.DataFrame(dev_fever_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm-zs3gcq6uy",
        "colab_type": "code",
        "outputId": "e189b8e0-f996-4ffe-cd8e-ee3862755919",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>predicted_label</th>\n",
              "      <th>predicted_evidence</th>\n",
              "      <th>evidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NOT ENOUGH INFO</td>\n",
              "      <td>NOT ENOUGH INFO</td>\n",
              "      <td>[[Louisiana_Football_Field, 0], [Mackay_Region...</td>\n",
              "      <td>[[[108548, None, None, None]]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NOT ENOUGH INFO</td>\n",
              "      <td>NOT ENOUGH INFO</td>\n",
              "      <td>[[Tilda_Swinton, 0], [Tilda_Swinton, 4], [Tild...</td>\n",
              "      <td>[[[227768, None, None, None]]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[[Soul_Food, 17], [Soul_Food_-LRB-TV_series-RR...</td>\n",
              "      <td>[[[289914, 283015, Soul_Food_-LRB-film-RRB-, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NOT ENOUGH INFO</td>\n",
              "      <td>NOT ENOUGH INFO</td>\n",
              "      <td>[[Macedonia,_New_Jersey, 0], [Anne_Rice, 0], [...</td>\n",
              "      <td>[[[191656, None, None, None], [191657, None, N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>REFUTES</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[[List_of_films_based_on_English-language_comi...</td>\n",
              "      <td>[[[131371, 146144, Telemundo, 0]], [[131371, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19993</th>\n",
              "      <td>REFUTES</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[[Hermit, 0], [Matthew_the_Hermit, 0], [Hermit...</td>\n",
              "      <td>[[[15450, 19262, Hermit_crab, 0], [15450, 1926...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19994</th>\n",
              "      <td>REFUTES</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[[Michael_Hutchence, 0], [Michael_Hutchence, 1...</td>\n",
              "      <td>[[[168967, 182663, Michael_Hutchence, 15]]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>SUPPORTS</td>\n",
              "      <td>[[Greece_-LRB-town-RRB-,_New_York, 5], [Greece...</td>\n",
              "      <td>[[[104709, 118125, Cyclades, 0]]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>NOT ENOUGH INFO</td>\n",
              "      <td>NOT ENOUGH INFO</td>\n",
              "      <td>[[Theresa_May_-LRB-disambiguation-RRB-, 0], [T...</td>\n",
              "      <td>[[[131223, None, None, None]]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>REFUTES</td>\n",
              "      <td>REFUTES</td>\n",
              "      <td>[[Trouble_-LRB-Trampled_by_Turtles_album-RRB-,...</td>\n",
              "      <td>[[[99015, 112132, Trouble_with_the_Curve, 0]],...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>19998 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 label  ...                                           evidence\n",
              "0      NOT ENOUGH INFO  ...                     [[[108548, None, None, None]]]\n",
              "1      NOT ENOUGH INFO  ...                     [[[227768, None, None, None]]]\n",
              "2             SUPPORTS  ...  [[[289914, 283015, Soul_Food_-LRB-film-RRB-, 0...\n",
              "3      NOT ENOUGH INFO  ...  [[[191656, None, None, None], [191657, None, N...\n",
              "4              REFUTES  ...  [[[131371, 146144, Telemundo, 0]], [[131371, 1...\n",
              "...                ...  ...                                                ...\n",
              "19993          REFUTES  ...  [[[15450, 19262, Hermit_crab, 0], [15450, 1926...\n",
              "19994          REFUTES  ...        [[[168967, 182663, Michael_Hutchence, 15]]]\n",
              "19995         SUPPORTS  ...                  [[[104709, 118125, Cyclades, 0]]]\n",
              "19996  NOT ENOUGH INFO  ...                     [[[131223, None, None, None]]]\n",
              "19997          REFUTES  ...  [[[99015, 112132, Trouble_with_the_Curve, 0]],...\n",
              "\n",
              "[19998 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPtm-LWYq7LY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = x[x['predicted_label'] == 'NOT ENOUGH INFO'].index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXRsStFprFNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.loc[i, 'predicted_evidence'] = [[]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUo4XRHZrL6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = x.to_dict('records')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTq7rG0Crf_d",
        "colab_type": "code",
        "outputId": "446dc6d6-53da-494a-b951-af66abfbe613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "y[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'evidence': [[[108548, None, None, None]]],\n",
              " 'label': 'NOT ENOUGH INFO',\n",
              " 'predicted_evidence': [],\n",
              " 'predicted_label': 'NOT ENOUGH INFO'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsXGik-erg7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}