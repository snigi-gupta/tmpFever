# -*- coding: utf-8 -*-
"""Generate_TrainingData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-idL7LaFaPyN3BvKw5O8TpWGdKyWGZ7f
"""

!pip install Unidecode

import pandas as pd
import numpy as np
import glob
import json
import unidecode

# %load_ext autotime

from google.colab import drive
drive.mount('/content/drive')

"""# Pre-Processing - Training Data"""

wiki_pages = glob.glob('/content/drive/My Drive/NLPWikiData/wiki-pages/*')
# wiki_pages = glob.glob('/content/drive/My Drive/NLPWikiData/wiki-pages/wiki-045.jsonl')

df = pd.DataFrame()

train_json_path = "/content/drive/My Drive/NLPWikiData/train.jsonl"
dev_json_path = "/content/drive/My Drive/NLPWikiData/dev.jsonl"
devdf = pd.read_json(train_json_path, lines=True, orient='records')
devdf = devdf[devdf['verifiable'] == 'VERIFIABLE']

devdf

values = devdf['evidence'].explode().explode()
index = values.index

values

index

train = pd.DataFrame(list(values))

train

train['claim'] = devdf['claim'][index].reset_index()['claim']
train['id'] = devdf['id'][index].reset_index()['id']
train['label'] = devdf['label'][index].reset_index()['label']
del train[0]
del train[1]
train.columns = ['id', 'sentence', 'claim', 'claim_id', 'label']

train

train['match_id'] = train['id'].apply(lambda x: unidecode.unidecode(x))

# train.iloc[219683]

# def split_into_id_and_lines(s):
#     lines = s.split('\n')
#     result = []
#     for line in lines:
#         sentence_id, text = line.split('\t', 1)
#         result.append([sentence_id, text])
#     return result

def merge_dfs(pages, wiki_dump, train, i):
    # tmp = wiki_dump[wiki_dump['id'].isin(pages)]
    # # tmp['lines'] = tmp['lines'].apply(lambda x: split_into_id_and_lines(x))
    # train = train.merge(tmp, how='left', left_on='id', right_on='id')
    # if i > 0:
    #     train['text'] = train['text_x'].fillna(train['text_y'])
    #     train['lines'] = train['lines_x'].fillna(train['lines_y'])
    #     train = train.drop(labels=['text_x', 'text_y', 'lines_x', 'lines_y'], axis=1)
    # return train


    tmp = wiki_dump[wiki_dump['match_id'].isin(pages)]
    # tmp['lines'] = tmp['lines'].apply(lambda x: split_into_id_and_lines(x))
    train = train.merge(tmp, how='left', left_on='match_id', right_on='match_id')
    if i > 0:
        train['id'] = train['id_x'].fillna(train['id_y'])
        train['text'] = train['text_x'].fillna(train['text_y'])
        train['lines'] = train['lines_x'].fillna(train['lines_y'])
        train = train.drop(labels=['text_x', 'text_y', 'lines_x', 'lines_y', 'id_x', 'id_y'], axis=1)
    return train

batch_size = 10
wiki_pages_chunks = np.array_split(wiki_pages, batch_size)
i = 0
for chunks in wiki_pages_chunks:
    df = pd.DataFrame()
    for f in chunks:
        d = pd.read_json(f, orient='records', lines=True)
        df = df.append(d, ignore_index=True)
        print(f'read {f}')
    del d
    # train = merge_dfs(train['id'], df, train, i)
    df['match_id'] = df['id'].apply(lambda x: unidecode.unidecode(x))
    train = merge_dfs(train['match_id'], df, train, i)
    
    print(f'done chunk {i}')
    i += 1
    # if i ==2:
      # break

train.drop(labels=['text'], axis=1, inplace=True)

train

path = "/content/drive/My Drive/NLPWikiData/raw2_dev_data.json"
train.to_json(path, orient='index')

raw_data = pd.read_json("/content/drive/My Drive/NLPWikiData/raw2_dev_data.json",orient='index')

raw_data.head(2)

raw_data.shape

def rule_split(lines_b):
    '''
    Important function!
    This function clean the lines in original preprocessed wiki Pages.
    :param lines_b:
    :return: a list of sentence (responded to each sentence)
    '''
    lines = []

    i = 0
    while True:
        start = f'\n{i}\t'
        end = f'\n{i + 1}\t'
        start_index = lines_b.find(start)
        end_index = lines_b.find(end)

        if end_index == -1:
            extra = f'\n{i + 2}\t'
            extra_1 = f'\n{i + 3}\t'
            extra_2 = f'\n{i + 4}\t'

            # print(lines_b[start_index:])
            lines.append(lines_b[start_index:])
            # print('What?')
            # print(lines_b)
            # print(extra, extra_1)
            # print(lines_b.find(extra))
            # print(lines_b.find(extra_1))
            # print(not (lines_b.find(extra) == -1 and lines_b.find(extra_1) == -1))

            if not (lines_b.find(extra) == -1
                    and lines_b.find(extra_1) == -1
                    and lines_b.find(extra_2) == -1):
                print(lines_b)
                print(extra, extra_1)
                print('Error')
            break

        lines.append(lines_b[start_index:end_index])
        i += 1

    return lines

def lines_to_items(lines):
    lines_list = []

    for i, line in enumerate(lines):
        line_item = dict()

        line_item_list = line.split('\t')

        line_num = line_item_list[0]
        # print(line_num)
        if not line_num.isdigit():
            print("None digit")
            print(lines)
        else:
            line_num = int(line_num)

        if int(line_num) != i:
            print("Line num mismath")
            print(int(line_num), i)

        line_item['line_num'] = line_num
        line_item['sentence'] = []
        line_item['h_links'] = []

        if len(line_item_list) <= 1:
            lines_list.append(line_item)
            continue

        sent = line_item_list[1].strip()
        h_links = line_item_list[2:]

        if 'thumb' in h_links:
            h_links = []
        else:
            h_links = list(filter(lambda x: len(x) > 0, h_links))

        line_item['sentence'] = sent
        line_item['h_links'] = h_links
        # print(line_num, sent)
        # print(len(h_links))
        # print(sent)
        # assert sent[-1] == '.'

        if len(h_links) % 2 != 0:
            
            for w in lines:
                print(w)
            print("Term mod 2 != 0")

            print("List:", line_item_list)
            print(line_num, sent)
            print(h_links)
            print()

        lines_list.append(line_item)

    # print(len(lines_list), lines_list[-1]['line_num'] + 1)
    # assert len(lines_list) == int(lines_list[-1]['line_num'] + 1)
    # string_line_item = json.dumps(lines_list)
    return lines_list

from tqdm import tqdm

processed_data = []
for data in tqdm(raw_data.itertuples()):
  # print(data)
  try:
    lines_with_slash_n = '\n'+data[7]
  except:
    print("Data:  ", data)
    continue

  lines = rule_split(lines_with_slash_n)
  lines = [line.strip().replace('\n', '') for line in lines]
  
  if len(lines) == 1 and lines[0] == '':
      lines = ['0']
  all_sentences = lines_to_items(lines)

  for i,sentence in enumerate(all_sentences):
    # temp_dict['doc_name'] = data[1]
    if len(sentence['sentence']) <3:
      continue
    temp_dict = dict()
    temp_dict['claim_id'] = data[3]
    temp_dict['claim'] = data[2]
    temp_dict['sentence'] = sentence['sentence']
    temp_dict['label'] = data[4]
    temp_dict['sent_label'] = sentence['line_num'] == data[1]
    # only for dev data
    temp_dict['sentno'] = sentence['line_num']
    temp_dict['doc_id'] = data[6]

    processed_data.append(temp_dict)

data

lines[1]

all_sentences

processed_data

new_df = pd.DataFrame(processed_data)
# only for dev data
new_df['org_sentence'] = new_df['sentence']

new_df.keys()

# new_df.claim_id.astype(str).str

# only for dev data
seperator = "{#--#}"
new_df['docid_claimid_sentno'] = new_df.doc_id + seperator + new_df.claim_id.astype(str) + seperator + new_df.sentno.astype(str)
new_df.drop(labels=['doc_id', 'sentno'], axis=1, inplace=True)
new_df = new_df.drop_duplicates().reset_index(drop=True)

new_df.to_csv("/content/drive/My Drive/NLPWikiData/processed2_dev_data.csv",index=False)

done_data = pd.read_csv("/content/drive/My Drive/NLPWikiData/processed2_dev_data.csv")
# training_data.groupby('claim_id').sum()

done_data.sent_label.value_counts()

done_data.head(8)

import pandas as pd

file_data_path="/content/drive/My Drive/NLPWikiData/sen_preds_output2.jsonl"
temp_df = pd.read_json(file_data_path, lines=True, orient='records')
temp2_df = pd.read_csv("/content/drive/My Drive/NLPWikiData/fever_data_output.csv")

temp_df

temp2_df

temp2_df.evidence



"""# Pre-Processing Test Data"""

wiki_pages = glob.glob('/content/drive/My Drive/NLPWikiData/wiki-pages/*')
test_json_path = "/content/drive/My Drive/NLPWikiData/doc_ret_test3.jsonl"

test_df = pd.DataFrame()

testdf = pd.read_json(test_json_path, lines=True, orient='records')
# testdf = testdf[testdf['verifiable'] == 'VERIFIABLE']

testdf

testdf.dtypes

values = testdf['pred_evidence'].explode()
index = values.index

values

index

test = pd.DataFrame(list(values))

test

test['claim']=testdf['claim'][index].reset_index()['claim']

test['claim_id']=testdf['id'][index].reset_index()['id']

test

test['claim'] = testdf['claim'][index].reset_index()['claim']
test['claim_id'] = testdf['id'][index].reset_index()['id']
# test['id'] = testdf['pred_evidence'][index].reset_index()['pred_evidence']
# test['label'] = testdf['label'][index].reset_index()['label']
# del test[0]
# del test[1]
test.columns = ['id', 'claim', 'claim_id']

# test.id.dropna(inplace=True)

test.dtypes

def a(x):
  try:
    return unidecode.unidecode(x)
    
  except Exception as e:
    print(e, x)
    raise Exception

test['match_id'] = test['id'].apply(lambda x: unidecode.unidecode(x))

test

# def split_into_id_and_lines(s):
#     lines = s.split('\n')
#     result = []
#     for line in lines:
#         sentence_id, text = line.split('\t', 1)
#         result.append([sentence_id, text])
#     return result

def merge_dfs(pages, wiki_dump, test, i):
    # tmp = wiki_dump[wiki_dump['id'].isin(pages)]
    # # tmp['lines'] = tmp['lines'].apply(lambda x: split_into_id_and_lines(x))
    # train = train.merge(tmp, how='left', left_on='id', right_on='id')
    # if i > 0:
    #     train['text'] = train['text_x'].fillna(train['text_y'])
    #     train['lines'] = train['lines_x'].fillna(train['lines_y'])
    #     train = train.drop(labels=['text_x', 'text_y', 'lines_x', 'lines_y'], axis=1)
    # return train


    tmp = wiki_dump[wiki_dump['match_id'].isin(pages)]
    # tmp['lines'] = tmp['lines'].apply(lambda x: split_into_id_and_lines(x))
    test = test.merge(tmp, how='left', left_on='match_id', right_on='match_id')
    if i > 0:
        test['id'] = test['id_x'].fillna(test['id_y'])
        test['text'] = test['text_x'].fillna(test['text_y'])
        test['lines'] = test['lines_x'].fillna(test['lines_y'])
        test = test.drop(labels=['text_x', 'text_y', 'lines_x', 'lines_y', 'id_x', 'id_y'], axis=1)
    return test

batch_size = 10
wiki_pages_chunks = np.array_split(wiki_pages, batch_size)
i = 0
for chunks in wiki_pages_chunks:
    df = pd.DataFrame()
    for f in chunks:
        d = pd.read_json(f, orient='records', lines=True)
        df = df.append(d, ignore_index=True)
        print(f'read {f}')
    del d
    # train = merge_dfs(train['id'], df, train, i)
    df['match_id'] = df['id'].apply(lambda x: unidecode.unidecode(x))
    test = merge_dfs(test['match_id'], df, test, i)
    
    print(f'done chunk {i}')
    i += 1
    # if i ==2:
      # break

test.drop(labels=['text'], axis=1, inplace=True)

test

path = "/content/drive/My Drive/NLPWikiData/raw_test_data3.json"
test.to_json(path, orient='index')

raw_test_data = pd.read_json(path,orient='index')

raw_test_data.head(2)

from tqdm import tqdm

test_processed_data = []
for t_data in tqdm(raw_test_data.itertuples()):
  # print(data)
  try:
    lines_with_slash_n = '\n'+t_data[5]
  except:
    print("Data:  ", t_data)
    continue

  lines = rule_split(lines_with_slash_n)
  lines = [line.strip().replace('\n', '') for line in lines]
  
  if len(lines) == 1 and lines[0] == '':
      lines = ['0']
  all_sentences = lines_to_items(lines)

  for i,sentence in enumerate(all_sentences):
    # temp_dict['doc_name'] = data[1]
    if len(sentence['sentence']) <3:
      continue
    temp_dict = dict()
    temp_dict['claim_id'] = t_data[2]
    temp_dict['claim'] = t_data[1]
    temp_dict['sentence'] = sentence['sentence']
    # temp_dict['label'] = t_data[4]
    # temp_dict['sent_label'] = sentence['line_num'] == data[1]
    temp_dict['sent_label'] = False
    # only for dev data
    temp_dict['sentno'] = sentence['line_num']
    temp_dict['doc_id'] = t_data[4]

    test_processed_data.append(temp_dict)

test_processed_data

test_new_df = pd.DataFrame(test_processed_data)
# only for dev data
test_new_df['org_sentence'] = test_new_df['sentence']

test_new_df

# only for test data
seperator = "{#--#}"
test_new_df['docid_claimid_sentno'] = test_new_df.doc_id + seperator + test_new_df.claim_id.astype(str) + seperator + test_new_df.sentno.astype(str)
test_new_df.drop(labels=['doc_id', 'sentno'], axis=1, inplace=True)
test_new_df = test_new_df.drop_duplicates().reset_index(drop=True)

test_new_df.to_csv("/content/drive/My Drive/NLPWikiData/processed_test_data3.csv",index=False)

test_done_data = pd.read_csv("/content/drive/My Drive/NLPWikiData/processed_test_data3.csv")

test_done_data



"""# **Pre-Process - Dev Data**"""

wiki_pages = glob.glob('/content/drive/My Drive/NLPWikiData/wiki-pages/*')

df = pd.DataFrame()

dev_json_path = "/content/drive/My Drive/NLPWikiData/doc_ret_dev3.jsonl"
devdf = pd.read_json(dev_json_path, lines=True, orient='records')
# devdf = devdf[devdf['verifiable'] == 'VERIFIABLE']

devdf

values = devdf['pred_evidence'].explode()
index = values.index

values

index

dev = pd.DataFrame(list(values))

dev

# values2 = devdf['evidence'].explode().explode()
# index2 = values2.index

# dev = pd.DataFrame(list(values2))

# dev

dev['claim']=devdf['claim'][index].reset_index()['claim']

dev['claim_id']=devdf['id'][index].reset_index()['id']

dev

# dev['label'] = devdf['label'][index].reset_index()['label']

dev.columns = ['id', 'claim', 'claim_id']

dev

dev['match_id'] = dev['id'].apply(lambda x: unidecode.unidecode(x))

# def split_into_id_and_lines(s):
#     lines = s.split('\n')
#     result = []
#     for line in lines:
#         sentence_id, text = line.split('\t', 1)
#         result.append([sentence_id, text])
#     return result

def merge_dfs(pages, wiki_dump, dev, i):
    # tmp = wiki_dump[wiki_dump['id'].isin(pages)]
    # # tmp['lines'] = tmp['lines'].apply(lambda x: split_into_id_and_lines(x))
    # train = train.merge(tmp, how='left', left_on='id', right_on='id')
    # if i > 0:
    #     train['text'] = train['text_x'].fillna(train['text_y'])
    #     train['lines'] = train['lines_x'].fillna(train['lines_y'])
    #     train = train.drop(labels=['text_x', 'text_y', 'lines_x', 'lines_y'], axis=1)
    # return train


    tmp = wiki_dump[wiki_dump['match_id'].isin(pages)]
    # tmp['lines'] = tmp['lines'].apply(lambda x: split_into_id_and_lines(x))
    dev = dev.merge(tmp, how='left', left_on='match_id', right_on='match_id')
    if i > 0:
        dev['id'] = dev['id_x'].fillna(dev['id_y'])
        dev['text'] = dev['text_x'].fillna(dev['text_y'])
        dev['lines'] = dev['lines_x'].fillna(dev['lines_y'])
        dev = dev.drop(labels=['text_x', 'text_y', 'lines_x', 'lines_y', 'id_x', 'id_y'], axis=1)
    return dev

batch_size = 10
wiki_pages_chunks = np.array_split(wiki_pages, batch_size)
i = 0
for chunks in wiki_pages_chunks:
    df = pd.DataFrame()
    for f in chunks:
        d = pd.read_json(f, orient='records', lines=True)
        df = df.append(d, ignore_index=True)
        print(f'read {f}')
    del d
    # train = merge_dfs(train['id'], df, train, i)
    df['match_id'] = df['id'].apply(lambda x: unidecode.unidecode(x))
    dev = merge_dfs(dev['match_id'], df, dev, i)
    
    print(f'done chunk {i}')
    i += 1
    # if i ==2:
      # break

dev.drop(labels=['text'], axis=1, inplace=True)

dev

dev_path = "/content/drive/My Drive/NLPWikiData/raw_dev_data3.json"
dev.to_json(dev_path, orient='index')

raw_dev_data = pd.read_json(dev_path,orient='index')

raw_dev_data.head(2)

from tqdm import tqdm

dev_processed_data = []
for d_data in tqdm(raw_dev_data.itertuples()):
  # print(data)
  try:
    lines_with_slash_n = '\n'+d_data[5]
  except:
    print("Data:  ", d_data)
    continue

  lines = rule_split(lines_with_slash_n)
  lines = [line.strip().replace('\n', '') for line in lines]
  
  if len(lines) == 1 and lines[0] == '':
      lines = ['0']
  all_sentences = lines_to_items(lines)

  for i,sentence in enumerate(all_sentences):
    # temp_dict['doc_name'] = data[1]
    if len(sentence['sentence']) <3:
      continue
    temp_dict = dict()
    temp_dict['claim_id'] = d_data[2]
    temp_dict['claim'] = d_data[1]
    temp_dict['sentence'] = sentence['sentence']
    # temp_dict['label'] = d_data[4]
    # temp_dict['sent_label'] = sentence['line_num'] == data[1]
    # temp_dict['sent_label'] = False
    # only for dev/test data
    temp_dict['sentno'] = sentence['line_num']
    temp_dict['doc_id'] = d_data[4]

    dev_processed_data.append(temp_dict)

dev_new_df = pd.DataFrame(dev_processed_data)
# only for dev data
dev_new_df['org_sentence'] = dev_new_df['sentence']

dev_new_df

# only for test/dev data
seperator = "{#--#}"
dev_new_df['docid_claimid_sentno'] = dev_new_df.doc_id + seperator + dev_new_df.claim_id.astype(str) + seperator + dev_new_df.sentno.astype(str)
dev_new_df.drop(labels=['doc_id', 'sentno'], axis=1, inplace=True)
dev_new_df = dev_new_df.drop_duplicates().reset_index(drop=True)

dev_new_df

dev_new_df.to_csv("/content/drive/My Drive/NLPWikiData/processed_dev_data3.csv",index=False)

dev_done_data = pd.read_csv("/content/drive/My Drive/NLPWikiData/processed_dev_data3.csv")

dev_done_data

