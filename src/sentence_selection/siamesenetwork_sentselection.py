# -*- coding: utf-8 -*-
"""SiameseNetwork_SentSelection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GNICh56bYMtVQaTRdm8iHXd5MLqj8CC2
"""

!pip install fever-scorer

from google.colab import drive
drive.mount('/content/gdrive')

import torch
import spacy
from tqdm import tqdm
from torchtext import data
from torchtext import datasets
import torch.nn.functional as tnf
from fever.scorer import fever_score
import pandas as pd
import pdb
import dill

SEED = 1234

torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

TEXT = data.Field(include_lengths = True, tokenize='spacy')
LABEL = data.LabelField()
OTHER = data.RawField()
OTHER.is_target = False

trainset_fields = {"sent_label":("sent_label",LABEL), "sentence":("sentence",TEXT), "claim":("claim",TEXT)}
devset_fields = {"sent_label":("sent_label",LABEL), "sentence":("sentence",TEXT), "claim":("claim", TEXT), 
                 "org_sentence":("org_sentence",OTHER), "docid_claimid_sentno":("docid_claimid_sentno",OTHER)}

# set paths
train_path = "/content/gdrive/My Drive/NLPWikiData/processed_train_data2.csv"
processed_dev_path = "/content/gdrive/My Drive/NLPWikiData/processed_golddev_data2.csv" 
dev_path = "/content/gdrive/My Drive/NLPWikiData/dev.jsonl"
# sen_preds_output_path = "/content/gdrive/My Drive/NLPWikiData/sen_pred_train.jsonl"
vocabulary_path = "/content/gdrive/My Drive/"

trainset = data.TabularDataset(train_path, format="CSV", fields=trainset_fields, skip_header=False)
devset = data.TabularDataset(processed_dev_path, format="CSV", fields=devset_fields, skip_header=False)

print(len(trainset))
print(vars(trainset.examples[0]))
print(len(devset))
print(vars(devset.examples[0]))

TEXT.build_vocab(trainset,vectors="glove.6B.100d",unk_init=torch.Tensor.normal_)

LABEL.build_vocab(trainset)

# vocabulary of training data (same to be used for dev and test)
print(f"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}")
print(f"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}")

print(TEXT.vocab.freqs.most_common(20))
print(TEXT.vocab.itos[:10])
print(vars(LABEL.vocab))

with open(vocabulary_path + "TEXT_VOCAB_5EPOCH", mode="wb") as f:
    dill.dump(TEXT, f)
    print("Text Dumping Successfull")
with open(vocabulary_path + "LABEL_VOCAB_5EPOCH", mode="wb") as f:
    dill.dump(LABEL,f)
    print("Label Dumping Successfull")

with open(vocabulary_path+"TEXT_VOCAB_5EPOCH", "rb") as f:
    X = dill.load(f)
    print("Loaded Successfully")
with open(vocabulary_path+"LABEL_VOCAB_5EPOCH", "rb") as f:
    Y = dill.load(f)
    print("Loaded Successfully")

BATCH_SIZE=128
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("We are working with ", device)

train_iterator = data.BucketIterator(
    trainset, 
    batch_size = BATCH_SIZE,
    sort_within_batch = True,
    sort_key = lambda x: (len(x.claim)),
    device = device)
dev_iterator = data.BucketIterator(
    devset, 
    batch_size = BATCH_SIZE,
    sort_within_batch = True,
    sort_key = lambda x: (len(x.claim)),
    device = device)

import torch.nn as nn

class LSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, 
                 bidirectional, pad_idx):
        
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, bidirectional=bidirectional)
        self.fc = nn.Linear(hidden_dim*2*2, output_dim)

        # self.dropoutVar = nn.Dropout(dropout)
        
    def forward_again(self, text, text_lengths):
        
        # print(text)
        # text = [sent_len, batch_size]
        # print("Text_Shape:  ",text.shape)
        # print("Text_Length: ",text_lengths)
        # print("Text_Length_Shape: ",text_lengths.shape)

        output = self.embedding(text) #get embeddings
        pps = nn.utils.rnn.pack_padded_sequence(output, text_lengths, enforce_sorted=False) #perform packed padded sequence
        output2, (hiddenLSTM, cellLSTM) = self.lstm(pps) #lstm
        hidden = torch.cat((hiddenLSTM[-2,:,:], hiddenLSTM[-1,:,:]),1) #get concatenated hidden

        # print("Output:  ",output)
        # print("Output_Shape:  ",output.shape)
        
        # print("PPS:  ",pps)
        # print("PPS_Shape:  ",pps.shape)

        # print("Output2:  ",output2)
        # print("Output2_Shape:  ",output2.shape)
        
        # print("Hidden:  ",hidden)
        # print("Hidden_Shape:  ",hidden.shape)
        
        return hidden

    def forward(self, claims, sentences):
        claim_text = claims[0]
        claim_text_length = claims[1]
        sentence_text = sentences[0]
        sentence_text_length = sentences[1]

        claim_hidden = self.forward_again(claim_text, claim_text_length)
        sentence_hidden = self.forward_again(sentence_text, sentence_text_length)

        concatenated_hidden = torch.cat((claim_hidden,sentence_hidden), 1)

        return self.fc(concatenated_hidden)

INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 2
N_LAYERS = 1
BIDIRECTIONAL = True
# DROPOUT = 0.5
PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]

model = LSTM(INPUT_DIM, 
            EMBEDDING_DIM, 
            HIDDEN_DIM, 
            OUTPUT_DIM, 
            N_LAYERS, 
            BIDIRECTIONAL, 
            PAD_IDX)

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(model):,} trainable parameters')

pretrained_embeddings = TEXT.vocab.vectors

print(pretrained_embeddings.shape)

model.embedding.weight.data.copy_(pretrained_embeddings)

UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]

model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)
model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)

print(model.embedding.weight.data)

import torch.optim as optim

optimizer = optim.Adam(model.parameters())

criterion = nn.CrossEntropyLoss()

model = model.to(device)
criterion = criterion.to(device)

def get_score(probabilities, docid_claimid_sentno, org_sentence, dev_path):
    org_dev_data = pd.read_json(dev_path, lines=True)

    claim_dict = dict()

    for i,val in enumerate(docid_claimid_sentno):
        doc_id, claim_id, sentno = docid_claimid_sentno[i].split("{#--#}")
        claim_id = int(claim_id)
        if claim_id not in claim_dict:
            claim_dict[claim_id] = [{"probability": probabilities[i], "doc_id": doc_id, 
                                    "sentno": sentno, "org_sentence": org_sentence[i]}]
        else:
            claim_dict[claim_id].extend([{"probability": probabilities[i], "doc_id": doc_id, 
                                    "sentno": sentno, "org_sentence": org_sentence[i]}])
    
    file_data = []
    fever_data = []
    prob_count = 0
    # pdb.set_trace()
    for org_dev_claim_id, org_dev_claim_label, org_dev_claim, org_dev_evidence_list in zip(org_dev_data['id'], org_dev_data['label'], 
                                                           org_dev_data['claim'], org_dev_data['evidence']):
        temp_data = dict()
        fever_dict = dict()

        org_dev_claim_id = int(org_dev_claim_id)
        predicted_sentences = []
        if org_dev_claim_id not in claim_dict:
            # that claim id was not in the predictions, hence no predicted sentences
            predicted_sentences = []
        else:
            the_claim_dict = claim_dict[org_dev_claim_id]
            for value in the_claim_dict:
                #   if value['probability'] >= 0.5:
                # print (predicted_sentences)
                # print (value)
                prob_count +=1
                predicted_sentences.append([value['probability'], value['sentno'], value['org_sentence'], value['doc_id']])
                            
            sorted_predicted_sentences = sorted(predicted_sentences, key=lambda x: x[0], reverse=True)
                
            # for RTE .jsonl file
            temp_data['id'] = org_dev_claim_id
            temp_data['claim'] = org_dev_claim
            temp_data['sentences'] = [u[2] for u in sorted_predicted_sentences][:5]
            temp_data['page_ids'] = [v[3] for v in sorted_predicted_sentences][:5]
            temp_data['indices'] = [w[1] for w in sorted_predicted_sentences][:5]
            

            # for fever score
            fever_dict['label'] = org_dev_claim_label
            fever_dict['predicted_label'] = org_dev_claim_label
            fever_dict['predicted_evidence'] = [[x[3], int(x[1])] for x in sorted_predicted_sentences][:5]
            fever_dict['evidence'] = org_dev_evidence_list

            file_data.append(temp_data)
            fever_data.append(fever_dict)
    # pd.DataFrame(file_data).to_json(sen_preds_output_path, orient='records', lines=True)
    print('prob_count', prob_count)
    return file_data, fever_data

def evaluate(model, iterator, file):
  
    epoch_loss = 0

    # doc_ids = []
    # sentence_nos = []
    # claim_ids = []
    docid_claimid_sentno = []
    org_sentences = []
    predicted_sentences = []
    probabilities = []
    correct_predictions = 0
    total_claims = 0

    # epoch_acc = 0

  
  
    with torch.no_grad():
  
        for i, batch in enumerate(iterator):
            model.eval()
            
            claims, sentences = batch.claim, batch.sentence
            
            eval_predictions = model(claims, sentences)
            probability = tnf.softmax(eval_predictions, 1)
            correct_predictions += (torch.max(eval_predictions, 1)[1].view(batch.sent_label.size()) == batch.sent_label).sum().item()
            dev_loss = criterion(eval_predictions, batch.sent_label)
            
            epoch_loss += dev_loss.item()
            total_claims += batch.sent_label.size(0)
            # epoch_acc += acc.item()

            # predicted_sentences.extend(eval_predictions[:,1].tolist())
            probabilities.extend(probability[:,1].tolist())
            docid_claimid_sentno.extend(batch.docid_claimid_sentno)
            org_sentences.extend(batch.org_sentence)

        file_data, fever_data = get_score(probabilities, docid_claimid_sentno, org_sentences, dev_path)   
        average_accuracy = 100. * correct_predictions / total_claims
        print(f'Correct Predictions: {correct_predictions}')
        print(f'Total Claims: {total_claims}')
        print(f'Validation Loss: {epoch_loss/len(iterator)}')
        print(f'Average Accuracy: {average_accuracy}%')
        print(f'-----------------------------')

    return file_data, fever_data

import pdb


def train(model, optimizer, criterion, path, best_f1, epoch_num):
  
    if not model.training:
        model.train()

    epoch_loss = 0
  
    for i,batch in enumerate(train_iterator):
        model.train()
        
        optimizer.zero_grad()
        claims, sentences = batch.claim, batch.sentence
        predictions = model(claims, sentences)
        
        loss = criterion(predictions, batch.sent_label)
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        
        if (i+1)%5000 == 0:
            print(f'BATCH:  {i+1}')

    # if (i+1)%10000 == 0:
    #   pdb.set_trace()
    print("--------------------------------")  
    print(f'BATCH: {i+1}')
    print("loss", epoch_loss/(i+1))
    file_data, fever_data = evaluate(model, dev_iterator, dev_path)
    fever_val, accuracy, precision, recall, f1score = fever_score(fever_data)
    print(f'Fever Score: {fever_val} | Accuracy: {accuracy}')
    print(f'Precision: {precision} | Recall: {recall} | F1Score: {f1score}')

    # if f1score > best_f1:
    # best_f1 = f1score
    print(f'Saving Model. . . ')
    torch.save(model.state_dict(), model_path+f'{epoch_num}_{f1score:0.3f}.pt')
    print(f'Model Saved Successfully!')
    pd.DataFrame(fever_data).to_csv("/content/gdrive/My Drive/NLPWikiData/fever_data_output_E"+f'{epoch_num}.csv')
    pd.DataFrame(file_data).to_json("/content/gdrive/My Drive/NLPWikiData/sen_pred_train_E"+f'{epoch_num}.jsonl', orient='records', lines=True)
    print(f'jsonl file saved for RTE')
    print("--------------------------------")  
    print("--------------------------------")  
    

    return epoch_loss / len(train_iterator), best_f1

import time

def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

N_EPOCHS = 5
model_path = F"/content/gdrive/My Drive/sent_selec_E"
best_valid_loss = float('inf')
best_f1 = 0

for epoch in range(N_EPOCHS):

    start_time = time.time()
    
    train_loss, best_f1 = train(model, optimizer, criterion, model_path, best_f1, epoch)
    # valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)
    
    end_time = time.time()

    epoch_mins, epoch_secs = epoch_time(start_time, end_time)
    
    # if valid_loss < best_valid_loss:
        # best_valid_loss = valid_loss
        # torch.save(model.state_dict(), path)
    
    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
    # print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    # print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')



"""# **Model Testing**"""

test_path = "/content/gdrive/My Drive/NLPWikiData/NEWprocessed_test_data.csv"

model_path = "/content/gdrive/My Drive/sent_selec_0.42889917068716255.pt"
sen_pred_test_path = "/content/gdrive/My Drive/NLPWikiData/sen_pred_test.jsonl"

TEXT = data.Field(include_lengths = True, tokenize='spacy')
# LABEL = data.LabelField()
OTHER = data.RawField()
OTHER.is_target = False

testset_fields = {"sentence":("sentence",TEXT), "claim":("claim", TEXT), 
                 "org_sentence":("org_sentence",OTHER), "docid_claimid_sentno":("docid_claimid_sentno",OTHER)}

with open("/content/gdrive/My Drive/TEXT_VOCAB_5EPOCH", "rb") as f:
    TEST_TEXT = dill.load(f)
    print("Text Load Successfull")
with open("/content/gdrive/My Drive/LABEL_VOCAB_5EPOCH", "rb") as f:
    TEST_LABEL = dill.load(f)
    print("Label Load Successfull")

testset = data.TabularDataset(test_path, format="CSV", fields=testset_fields, skip_header=False)

print(len(testset))
print(vars(testset.examples[0]))

TEXT.build_vocab(testset)
# ,vectors="glove.6B.100d",unk_init=torch.Tensor.normal_)

LABEL.build_vocab(testset)

TEXT.vocab = TEST_TEXT.vocab
TEXT.vocab.itos = TEST_TEXT.vocab.itos
TEXT.vocab.stoi = TEST_TEXT.vocab.stoi

LABEL.vocab = TEST_LABEL.vocab
LABEL.vocab.itos = TEST_LABEL.vocab.itos
LABEL.vocab.stoi = TEST_LABEL.vocab.stoi

# vocabulary of training data (same to be used for dev and test)
print(f"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}")
print(f"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}")

print(TEXT.vocab.freqs.most_common(20))
print(TEXT.vocab.itos[:10])
print(vars(LABEL.vocab))

BATCH_SIZE=128
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("We are working with ", device)

test_iterator = data.BucketIterator(
    testset, 
    batch_size = BATCH_SIZE,
    sort_within_batch = True,
    sort_key = lambda x: (len(x.claim)),
    device = device)

INPUT_DIM = len(TEXT.vocab)
EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 2
N_LAYERS = 1
BIDIRECTIONAL = True
# DROPOUT = 0.5
PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]

model = LSTM(INPUT_DIM, 
            EMBEDDING_DIM, 
            HIDDEN_DIM, 
            OUTPUT_DIM, 
            N_LAYERS, 
            BIDIRECTIONAL, 
            PAD_IDX)

criterion = nn.CrossEntropyLoss()
model.load_state_dict(torch.load(model_path, map_location=device)) 
model = model.to(device)
criterion = criterion.to(device)

def get_score_test(probabilities, docid_claimid_sentno, org_sentence, test_path):
    org_test_data = pd.read_json(dev_path, lines=True)

    claim_dict = dict()

    for i,val in enumerate(docid_claimid_sentno):
        doc_id, claim_id, sentno = docid_claimid_sentno[i].split("{#--#}")
        claim_id = int(claim_id)
        if claim_id not in claim_dict:
            claim_dict[claim_id] = [{"probability": probabilities[i], "doc_id": doc_id, 
                                    "sentno": sentno, "org_sentence": org_sentence[i]}]
        else:
            claim_dict[claim_id].extend([{"probability": probabilities[i], "doc_id": doc_id, 
                                    "sentno": sentno, "org_sentence": org_sentence[i]}])
    
    file_data = []
    fever_data = []
    prob_count = 0
    # pdb.set_trace()
    for org_test_claim_id, org_test_claim, in zip(org_test_data['id'], org_test_data['claim']):
        temp_data = dict()
        fever_dict = dict()

        org_test_claim_id = int(org_test_claim_id)
        predicted_sentences = []
        if org_test_claim_id not in claim_dict:
            # that claim id was not in the predictions, hence no predicted sentences
            predicted_sentences = []
        else:
            the_claim_dict = claim_dict[org_test_claim_id]
            for value in the_claim_dict:
                #   if value['probability'] >= 0.5:
                # print (predicted_sentences)
                # print (value)
                prob_count +=1
                predicted_sentences.append([value['probability'], value['sentno'], value['org_sentence'], value['doc_id']])
                            
            sorted_predicted_sentences = sorted(predicted_sentences, key=lambda x: x[0], reverse=True)
                
            # for RTE .jsonl file
            temp_data['id'] = org_test_claim_id
            temp_data['claim'] = org_test_claim
            temp_data['sentences'] = [u[2] for u in sorted_predicted_sentences][:5]
            temp_data['page_ids'] = [v[3] for v in sorted_predicted_sentences][:5]
            temp_data['indices'] = [w[1] for w in sorted_predicted_sentences][:5]
            

            # for fever score
            # fever_dict['label'] = org_dev_claim_label
            # fever_dict['predicted_label'] = org_dev_claim_label
            # fever_dict['predicted_evidence'] = [[x[3], int(x[1])] for x in sorted_predicted_sentences][:5]
            # fever_dict['evidence'] = org_dev_evidence_list
            fever_dict = "No fever Data"

            file_data.append(temp_data)
            # fever_data.append(fever_dict)
    pd.DataFrame(file_data).to_json(sen_pred_test_path, orient='records', lines=True)
    print('prob_count', prob_count)
    return file_data, fever_data

test_file_data, test_fever_data = evaluate(model, test_iterator, test_path)
# test_fever_val, test_accuracy, test_precision, test_recall, f1score = test_fever_score(fever_data)
# print(f'Fever Score: {fever_val} | Accuracy: {accuracy}')
# print(f'Precision: {precision} | Recall: {recall} | F1Score: {f1score}')

